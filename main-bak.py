#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time    : 2025/12/5 15:13
# @Author  : hejun
"""
ä¸»ç¨‹åºå…¥å£ï¼šç™¾ä¸‡çº§åœ°å€ç›¸ä¼¼åº¦è¯†åˆ«ä¸èšç±»ç³»ç»Ÿ
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

import argparse
import pandas as pd
import numpy as np
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from config.config import Config
from core.address_normalizer import AddressStandardizationPipeline
from core.geocoding_integration import GeocodingCHNIntegration, MultiSourceGeocoder
from core.similarity_calculator import MultiDimensionalSimilarityCalculator
from core.clustering import AddressClustering
from utils.parallel_processor import ParallelProcessor, MemoryOptimizedProcessor
from utils.visualization import VisualizationTools
from utils.logger import setup_logging

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = setup_logging('main-bak.py').get_logger()


class AddressSimilaritySystem:
    """åœ°å€ç›¸ä¼¼åº¦ç³»ç»Ÿï¼ˆä¸»ç±»ï¼‰"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åœ°å€ç›¸ä¼¼åº¦ç³»ç»Ÿ

        Args:
            config: é…ç½®å­—å…¸ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or Config.ALGORITHM_CONFIG

        # åˆå§‹åŒ–å„æ¨¡å—
        self.parallel_processor = ParallelProcessor(
            n_jobs=self.config['performance']['n_jobs'],
            max_memory_gb=32
        )

        self.memory_optimizer = MemoryOptimizedProcessor(
            max_memory_gb=32
        )

        self.address_normalizer = AddressStandardizationPipeline(
            self.config['normalization']
        )

        # åœ°ç†ç¼–ç å™¨
        if self.config['geocoding']['use_geocoding_chn']:
            geocoding_path = self.config['geocoding'].get('geocoding_chn_path')
            self.geocoder = GeocodingCHNIntegration(data_path=geocoding_path)
        else:
            self.geocoder = MultiSourceGeocoder()

        # ç›¸ä¼¼åº¦è®¡ç®—å™¨
        self.similarity_calculator = MultiDimensionalSimilarityCalculator(
            self.config
        )

        # èšç±»å™¨
        self.clustering = AddressClustering(self.config)

        # å¯è§†åŒ–å·¥å…·
        self.visualization = VisualizationTools()

        # çŠ¶æ€è·Ÿè¸ª
        self.processing_stats = {
            'start_time': None,
            'end_time': None,
            'total_addresses': 0,
            'normalized_addresses': 0,
            'geocoded_addresses': 0,
            'candidate_pairs': 0,
            'similar_pairs': 0,
            'clusters_found': 0,
            'memory_peak_mb': 0
        }

    def load_data(self, input_file: str, sample_size: Optional[int] = None) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        Returns:
            åŠ è½½çš„DataFrame
        """
        logger.info(f"åŠ è½½æ•°æ®: {input_file}")

        # æ”¯æŒå¤šç§æ ¼å¼
        if input_file.endswith('.csv'):
            df = pd.read_csv(input_file, encoding='utf-8')
        elif input_file.endswith('.parquet'):
            df = pd.read_parquet(input_file)
        elif input_file.endswith('.xlsx') or input_file.endswith('.xls'):
            df = pd.read_excel(input_file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}")

        # é‡‡æ ·ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if sample_size and sample_size < len(df):
            df = df.sample(n=min(sample_size, len(df)), random_state=42)
            logger.info(f"é‡‡æ · {len(df)} æ¡æ•°æ®ç”¨äºæµ‹è¯•")

        self.processing_stats['total_addresses'] = len(df)

        return df

    def preprocess_addresses(self, df: pd.DataFrame,
                             address_column: str = 'address',
                             lon_column: Optional[str] = None,
                             lat_column: Optional[str] = None) -> pd.DataFrame:
        """
        é¢„å¤„ç†åœ°å€æ•°æ®

        Args:
            df: è¾“å…¥DataFrame
            address_column: åœ°å€åˆ—å
            lon_column: ç»åº¦åˆ—å
            lat_column: çº¬åº¦åˆ—å

        Returns:
            é¢„å¤„ç†åçš„DataFrame
        """
        logger.info("\n=== åœ°å€é¢„å¤„ç† ===")

        # 1. åœ°å€æ ‡å‡†åŒ–
        logger.info("1. åœ°å€æ ‡å‡†åŒ–...")
        df = self.address_normalizer.process_dataframe(df, address_column)
        self.processing_stats['normalized_addresses'] = df['success'].sum()

        # 2. åœ°ç†ç¼–ç ï¼ˆå¦‚æœæä¾›äº†ç»çº¬åº¦åˆ—ï¼Œåˆ™éªŒè¯ï¼›å¦åˆ™è¿›è¡Œåœ°ç†ç¼–ç ï¼‰
        logger.info("2. åœ°ç†ç¼–ç ...")

        if lon_column and lat_column and lon_column in df.columns and lat_column in df.columns:
            # å·²æœ‰ç»çº¬åº¦ï¼ŒéªŒè¯å’Œè¡¥å…¨
            df['longitude'] = df[lon_column].astype(float)
            df['latitude'] = df[lat_column].astype(float)

            # é€†åœ°ç†ç¼–ç è·å–ç»“æ„åŒ–ä¿¡æ¯
            logger.info("  é€†åœ°ç†ç¼–ç è·å–åœ°å€ä¿¡æ¯...")
            geocoded_results = self.parallel_processor.batch_process(
                list(zip(df['latitude'], df['longitude'])),
                lambda x: self.geocoder.reverse_geocode(x[0], x[1]),
                batch_size=5000,
                desc="é€†åœ°ç†ç¼–ç "
            )

            df['geo_info'] = geocoded_results

        else:
            # æ²¡æœ‰ç»çº¬åº¦ï¼Œè¿›è¡Œåœ°ç†ç¼–ç 
            logger.info("  æ­£å‘åœ°ç†ç¼–ç ...")
            geocoded_results = self.parallel_processor.batch_process(
                df['original'].tolist(),
                self.geocoder.geocode,
                batch_size=5000,
                desc="åœ°ç†ç¼–ç "
            )

            # æå–ç»çº¬åº¦
            df['longitude'] = [r.get('longitude') if r else None for r in geocoded_results]
            df['latitude'] = [r.get('latitude') if r else None for r in geocoded_results]
            df['geo_info'] = geocoded_results

        self.processing_stats['geocoded_addresses'] = df['longitude'].notna().sum()

        # 3. æ·»åŠ ID
        df['id'] = range(len(df))

        logger.info(f"é¢„å¤„ç†å®Œæˆ: {len(df)} æ¡åœ°å€")
        logger.info(f"  - æ ‡å‡†åŒ–æˆåŠŸ: {self.processing_stats['normalized_addresses']}")
        logger.info(f"  - åœ°ç†ç¼–ç æˆåŠŸ: {self.processing_stats['geocoded_addresses']}")

        return df

    def find_similar_candidates(self, df: pd.DataFrame) -> List[Tuple[int, int]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼å€™é€‰å¯¹

        Args:
            df: é¢„å¤„ç†åçš„DataFrame

        Returns:
            å€™é€‰å¯¹åˆ—è¡¨
        """
        logger.info("\n=== æŸ¥æ‰¾ç›¸ä¼¼å€™é€‰å¯¹ ===")

        # å°†æ•°æ®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        address_data = {}
        for _, row in df.iterrows():
            address_data[row['id']] = {
                'id': row['id'],
                'original': row['original'],
                'standardized': row['standardized'],
                'latitude': row['latitude'],
                'longitude': row['longitude'],
                'parsed': row.get('parsed', {}),
                'components': row.get('components', {}),
                'geo_info': row.get('geo_info', {})
            }

        # æ–¹æ³•1: ä½¿ç”¨LSHæŸ¥æ‰¾æ–‡æœ¬ç›¸ä¼¼çš„å€™é€‰å¯¹
        logger.info("1. ä½¿ç”¨LSHæŸ¥æ‰¾æ–‡æœ¬ç›¸ä¼¼å€™é€‰å¯¹...")
        text_candidates = self.similarity_calculator.find_similar_candidates_with_lsh(
            address_data,
            threshold=self.config['text_similarity']['lsh_threshold']
        )
        logger.info(f"   æ‰¾åˆ° {len(text_candidates)} ä¸ªæ–‡æœ¬ç›¸ä¼¼å€™é€‰å¯¹")

        # æ–¹æ³•2: åŸºäºè¡Œæ”¿åŒºåˆ’çš„å€™é€‰å¯¹
        logger.info("2. åŸºäºè¡Œæ”¿åŒºåˆ’æŸ¥æ‰¾å€™é€‰å¯¹...")
        admin_candidates = self._find_admin_candidates(address_data)
        logger.info(f"   æ‰¾åˆ° {len(admin_candidates)} ä¸ªè¡Œæ”¿åŒºåˆ’å€™é€‰å¯¹")

        # æ–¹æ³•3: åŸºäºåœ°ç†ç©ºé—´çš„å€™é€‰å¯¹
        logger.info("3. åŸºäºåœ°ç†ç©ºé—´æŸ¥æ‰¾å€™é€‰å¯¹...")
        spatial_candidates = self._find_spatial_candidates(address_data)
        logger.info(f"   æ‰¾åˆ° {len(spatial_candidates)} ä¸ªç©ºé—´å€™é€‰å¯¹")

        # åˆå¹¶æ‰€æœ‰å€™é€‰å¯¹ï¼ˆå»é‡ï¼‰
        all_candidates = set(text_candidates) | set(admin_candidates) | set(spatial_candidates)
        self.processing_stats['candidate_pairs'] = len(all_candidates)

        logger.info(f"å€™é€‰å¯¹æ€»æ•°: {len(all_candidates):,}")

        return list(all_candidates)

    def _find_admin_candidates(self, address_data: Dict[int, Dict[str, Any]]) -> List[Tuple[int, int]]:
        """åŸºäºè¡Œæ”¿åŒºåˆ’æŸ¥æ‰¾å€™é€‰å¯¹"""
        candidates = set()

        # æŒ‰çœä»½-åŸå¸‚-åŒºå¿åˆ†ç»„
        admin_groups = {}

        for addr_id, addr_info in address_data.items():
            parsed = addr_info.get('parsed', {})
            admin_key = (
                parsed.get('province', ''),
                parsed.get('city', ''),
                parsed.get('district', '')
            )

            if admin_key not in admin_groups:
                admin_groups[admin_key] = []
            admin_groups[admin_key].append(addr_id)

        # åŒç»„å†…çš„åœ°å€ä½œä¸ºå€™é€‰å¯¹
        for group in admin_groups.values():
            if len(group) > 1:
                for i in range(len(group)):
                    for j in range(i + 1, len(group)):
                        candidates.add((min(group[i], group[j]), max(group[i], group[j])))

        return list(candidates)

    def _find_spatial_candidates(self, address_data: Dict[int, Dict[str, Any]]) -> List[Tuple[int, int]]:
        """åŸºäºåœ°ç†ç©ºé—´æŸ¥æ‰¾å€™é€‰å¯¹"""
        candidates = set()

        # æå–æœ‰åæ ‡çš„åœ°å€
        addresses_with_coords = []
        for addr_id, addr_info in address_data.items():
            lat = addr_info.get('latitude')
            lon = addr_info.get('longitude')
            if lat is not None and lon is not None:
                addresses_with_coords.append((addr_id, lat, lon))

        if len(addresses_with_coords) < 2:
            return []

        logger.info(f"   æœ‰åæ ‡çš„åœ°å€: {len(addresses_with_coords)}")

        # ä½¿ç”¨ç½‘æ ¼åˆ’åˆ†ç©ºé—´ï¼ŒåŠ é€ŸæŸ¥æ‰¾
        grid_size = 0.1  # çº¦10å…¬é‡Œ

        # åˆ›å»ºç©ºé—´ç½‘æ ¼
        grid = {}
        for addr_id, lat, lon in addresses_with_coords:
            grid_x = int(lon / grid_size)
            grid_y = int(lat / grid_size)
            grid_key = (grid_x, grid_y)

            if grid_key not in grid:
                grid[grid_key] = []
            grid[grid_key].append((addr_id, lat, lon))

        # åœ¨æ¯ä¸ªç½‘æ ¼åŠå…¶ç›¸é‚»ç½‘æ ¼ä¸­æŸ¥æ‰¾å€™é€‰å¯¹
        max_distance_km = self.config['spatial_similarity']['max_search_radius_km']

        for (grid_x, grid_y), addresses in grid.items():
            # æ£€æŸ¥å½“å‰ç½‘æ ¼å’Œç›¸é‚»ç½‘æ ¼
            for dx in [-1, 0, 1]:
                for dy in [-1, 0, 1]:
                    neighbor_key = (grid_x + dx, grid_y + dy)

                    if neighbor_key in grid:
                        # æ¯”è¾ƒå½“å‰ç½‘æ ¼å’Œç›¸é‚»ç½‘æ ¼ä¸­çš„åœ°å€
                        for addr1_id, lat1, lon1 in addresses:
                            for addr2_id, lat2, lon2 in grid[neighbor_key]:
                                if addr1_id < addr2_id:  # é¿å…é‡å¤
                                    # å¿«é€Ÿè·ç¦»ä¼°ç®—
                                    distance_approx = self._approx_distance(lat1, lon1, lat2, lon2)

                                    if distance_approx <= max_distance_km * 1.5:  # å®½æ¾é˜ˆå€¼
                                        candidates.add((addr1_id, addr2_id))

        return list(candidates)

    def _approx_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """å¿«é€Ÿä¼°ç®—è·ç¦»ï¼ˆå…¬é‡Œï¼‰"""
        # ç®€å•ä¼°ç®—ï¼Œæ¯åº¦çº¬åº¦çº¦111å…¬é‡Œï¼Œæ¯åº¦ç»åº¦çº¦111*cos(lat)å…¬é‡Œ
        dlat = abs(lat1 - lat2) * 111
        dlon = abs(lon1 - lon2) * 111 * max(0.5, np.cos(np.radians((lat1 + lat2) / 2)))
        return np.sqrt(dlat ** 2 + dlon ** 2)

    def calculate_similarities(self, df: pd.DataFrame,
                               candidate_pairs: List[Tuple[int, int]]) -> List[Dict[str, Any]]:
        """
        è®¡ç®—ç›¸ä¼¼åº¦

        Args:
            df: é¢„å¤„ç†åçš„DataFrame
            candidate_pairs: å€™é€‰å¯¹åˆ—è¡¨

        Returns:
            ç›¸ä¼¼åº¦ç»“æœåˆ—è¡¨
        """
        logger.info("\n=== è®¡ç®—ç›¸ä¼¼åº¦ ===")

        # å‡†å¤‡åœ°å€æ•°æ®
        address_data = {}
        for _, row in df.iterrows():
            address_data[row['id']] = {
                'id': row['id'],
                'original': row['original'],
                'standardized': row['standardized'],
                'latitude': row['latitude'],
                'longitude': row['longitude'],
                'parsed': row.get('parsed', {}),
                'components': row.get('components', {}),
                'geo_info': row.get('geo_info', {})
            }

        # å‡†å¤‡å€™é€‰å¯¹æ•°æ®
        candidate_pairs_with_info = [
            (id1, id2, {'source': 'candidate'})
            for id1, id2 in candidate_pairs
        ]

        # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
        logger.info(f"è®¡ç®— {len(candidate_pairs):,} ä¸ªå€™é€‰å¯¹çš„ç›¸ä¼¼åº¦...")

        similarity_results = self.similarity_calculator.batch_calculate_similarities(
            candidate_pairs_with_info,
            address_data,
            n_jobs=self.config['performance']['n_jobs']
        )

        # è¿‡æ»¤è¾¾åˆ°é˜ˆå€¼çš„ç›¸ä¼¼å¯¹
        threshold = self.config['clustering']['similarity_threshold']
        similar_pairs = [r for r in similarity_results
                         if r['comprehensive_similarity'] >= threshold]

        self.processing_stats['similar_pairs'] = len(similar_pairs)

        logger.info(f"ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ:")
        logger.info(f"  - å€™é€‰å¯¹æ€»æ•°: {len(candidate_pairs):,}")
        logger.info(f"  - ç›¸ä¼¼å¯¹æ•°é‡: {len(similar_pairs):,}")
        logger.info(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {threshold}")

        return similar_pairs

    def cluster_addresses(self, df: pd.DataFrame,
                          similarity_pairs: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        åœ°å€èšç±»

        Args:
            df: é¢„å¤„ç†åçš„DataFrame
            similarity_pairs: ç›¸ä¼¼åº¦å¯¹åˆ—è¡¨

        Returns:
            èšç±»åçš„DataFrame
        """
        logger.info("\n=== åœ°å€èšç±» ===")

        # åˆ›å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        logger.info("1. åˆ›å»ºç›¸ä¼¼åº¦çŸ©é˜µ...")
        address_ids = df['id'].tolist()

        address_data_list = []
        for _, row in df.iterrows():
            address_data_list.append({
                'id': row['id'],
                'original': row['original'],
                'standardized': row['standardized'],
                'latitude': row['latitude'],
                'longitude': row['longitude'],
                'parsed': row.get('parsed', {}),
                'components': row.get('components', {})
            })

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        address_data_dict = {item['id']: item for item in address_data_list}

        similarity_matrix = self.similarity_calculator.create_similarity_matrix(
            address_ids, address_data_dict, similarity_pairs
        )

        # å‡†å¤‡åæ ‡æ•°æ®
        coordinates = []
        for addr_id in address_ids:
            addr_info = address_data_dict[addr_id]
            if (addr_info.get('latitude') is not None and
                    addr_info.get('longitude') is not None):
                coordinates.append([addr_info['latitude'], addr_info['longitude']])
            else:
                coordinates.append(None)

        coordinates_array = np.array([c if c is not None else [np.nan, np.nan]
                                      for c in coordinates])

        # èšç±»
        logger.info("2. æ‰§è¡Œèšç±»...")

        if self.config['clustering'].get('use_spatial_constraints', True):
            # å¸¦ç©ºé—´çº¦æŸçš„èšç±»
            labels = self.clustering.cluster_with_spatial_constraints(
                similarity_matrix,
                coordinates_array,
                similarity_threshold=self.config['clustering']['similarity_threshold'],
                spatial_threshold_km=self.config['clustering']['max_cluster_radius_km']
            )
        else:
            # ä»…åŸºäºç›¸ä¼¼åº¦çš„èšç±»
            labels = self.clustering.cluster_by_connected_components(
                similarity_matrix,
                threshold=self.config['clustering']['similarity_threshold']
            )

        # åˆå¹¶å°èšç±»
        logger.info("3. ä¼˜åŒ–èšç±»ç»“æœ...")
        labels = self.clustering.merge_small_clusters(
            labels,
            min_size=self.config['clustering']['min_cluster_size']
        )

        # è®¡ç®—èšç±»è´¨é‡
        quality = self.clustering.calculate_cluster_quality(labels, similarity_matrix)

        logger.info(f"èšç±»å®Œæˆ:")
        logger.info(f"  - èšç±»æ•°é‡: {quality['n_clusters']}")
        logger.info(f"  - ç±»å†…å¹³å‡ç›¸ä¼¼åº¦: {quality['avg_similarity_within']:.3f}")
        logger.info(f"  - ç±»é—´å¹³å‡ç›¸ä¼¼åº¦: {quality['avg_similarity_between']:.3f}")
        logger.info(f"  - è½®å»“ç³»æ•°: {quality['silhouette_score']:.3f}")

        # æ·»åŠ èšç±»æ ‡ç­¾åˆ°DataFrame
        df['cluster_id'] = labels

        self.processing_stats['clusters_found'] = quality['n_clusters']

        return df, quality

    def generate_results(self, df: pd.DataFrame,
                         similarity_pairs: List[Dict[str, Any]],
                         output_prefix: str):
        """
        ç”Ÿæˆç»“æœæ–‡ä»¶

        Args:
            df: èšç±»åçš„DataFrame
            similarity_pairs: ç›¸ä¼¼åº¦å¯¹åˆ—è¡¨
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
        """
        logger.info("\n=== ç”Ÿæˆç»“æœæ–‡ä»¶ ===")

        output_dir = Config.OUTPUT_DIR
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. ä¿å­˜èšç±»ç»“æœ
        cluster_file = output_dir / f"{output_prefix}_clusters_{timestamp}.csv"
        df.to_csv(cluster_file, index=False, encoding='utf-8-sig')
        logger.info(f"1. èšç±»ç»“æœä¿å­˜åˆ°: {cluster_file}")

        # 2. ä¿å­˜èšç±»æ‘˜è¦
        cluster_summary = self.clustering.create_cluster_summary(
            df['cluster_id'].values,
            df.to_dict('records')
        )

        summary_file = output_dir / f"{output_prefix}_cluster_summary_{timestamp}.csv"
        cluster_summary.to_csv(summary_file, index=False, encoding='utf-8-sig')
        logger.info(f"2. èšç±»æ‘˜è¦ä¿å­˜åˆ°: {summary_file}")

        # 3. ä¿å­˜ç›¸ä¼¼åº¦å¯¹
        similarity_df = pd.DataFrame(similarity_pairs)
        similarity_file = output_dir / f"{output_prefix}_similarities_{timestamp}.csv"
        similarity_df.to_csv(similarity_file, index=False, encoding='utf-8-sig')
        logger.info(f"3. ç›¸ä¼¼åº¦å¯¹ä¿å­˜åˆ°: {similarity_file}")

        # 4. ä¿å­˜å¤„ç†ç»Ÿè®¡
        self.processing_stats['end_time'] = time.time()
        duration = self.processing_stats['end_time'] - self.processing_stats['start_time']

        stats = {
            'processing_stats': self.processing_stats,
            'duration_seconds': duration,
            'addresses_per_second': self.processing_stats['total_addresses'] / duration,
            'config': self.config,
            'quality_metrics': self.clustering.calculate_cluster_quality(
                df['cluster_id'].values,
                self.similarity_calculator.create_similarity_matrix(
                    df['id'].tolist(),
                    {row['id']: row for _, row in df.iterrows()},
                    similarity_pairs
                )
            ),
            'geocoding_stats': self.geocoder.get_stats() if hasattr(self.geocoder, 'get_stats') else {}
        }

        stats_file = output_dir / f"{output_prefix}_processing_stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"4. å¤„ç†ç»Ÿè®¡ä¿å­˜åˆ°: {stats_file}")

        # 5. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        try:
            vis_file = output_dir / f"{output_prefix}_visualization_{timestamp}.html"
            self.visualization.create_interactive_report(
                df, similarity_df, str(vis_file)
            )
            logger.info(f"5. å¯è§†åŒ–æŠ¥å‘Šä¿å­˜åˆ°: {vis_file}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šå¤±è´¥: {e}")

        logger.infologger.info(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def run_pipeline(self, input_file: str,
                     output_prefix: str = "address_similarity",
                     address_column: str = "address",
                     lon_column: Optional[str] = None,
                     lat_column: Optional[str] = None,
                     sample_size: Optional[int] = None):
        """
        è¿è¡Œå®Œæ•´å¤„ç†ç®¡é“

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
            address_column: åœ°å€åˆ—å
            lon_column: ç»åº¦åˆ—å
            lat_column: çº¬åº¦åˆ—å
            sample_size: é‡‡æ ·å¤§å°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        logger.info("=" * 60)
        logger.info("ç™¾ä¸‡çº§åœ°å€ç›¸ä¼¼åº¦è¯†åˆ«ä¸èšç±»ç³»ç»Ÿ")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now()}")
        logger.info("=" * 60)

        self.processing_stats['start_time'] = time.time()

        try:
            # 1. åŠ è½½æ•°æ®
            df = self.load_data(input_file, sample_size)

            # 2. é¢„å¤„ç†
            df = self.preprocess_addresses(df, address_column, lon_column, lat_column)

            # 3. æŸ¥æ‰¾ç›¸ä¼¼å€™é€‰å¯¹
            candidate_pairs = self.find_similar_candidates(df)

            # 4. è®¡ç®—ç›¸ä¼¼åº¦
            similarity_pairs = self.calculate_similarities(df, candidate_pairs)

            # 5. èšç±»
            df, quality = self.cluster_addresses(df, similarity_pairs)

            # 6. ç”Ÿæˆç»“æœ
            self.generate_results(df, similarity_pairs, output_prefix)

            # 7. æ‰“å°æ‘˜è¦
            self.print_summary(df, quality)

        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raiselogger.info

    def print_summary(self, df: pd.DataFrame, quality: Dict[str, Any]):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        duration = time.time() - self.processing_stats['start_time']

        logger.info("\n" + "=" * 60)
        logger.info("å¤„ç†å®Œæˆ!")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
        logger.info(f"   æ€»åœ°å€æ•°: {self.processing_stats['total_addresses']:,}")
        logger.info(f"   æ ‡å‡†åŒ–æˆåŠŸ: {self.processing_stats['normalized_addresses']:,}")
        logger.info(f"   åœ°ç†ç¼–ç æˆåŠŸ: {self.processing_stats['geocoded_addresses']:,}")
        logger.info(f"   å€™é€‰å¯¹æ•°é‡: {self.processing_stats['candidate_pairs']:,}")
        logger.info(f"   ç›¸ä¼¼å¯¹æ•°é‡: {self.processing_stats['similar_pairs']:,}")
        logger.info(f"   èšç±»æ•°é‡: {self.processing_stats['clusters_found']:,}")
        logger.info(f"   å¤„ç†æ—¶é—´: {duration:.2f}ç§’")
        logger.info(f"   å¤„ç†é€Ÿåº¦: {self.processing_stats['total_addresses'] / duration:.1f} åœ°å€/ç§’")
        logger.info(f"\nğŸ¯ èšç±»è´¨é‡:")
        logger.info(f"   è½®å»“ç³»æ•°: {quality.get('silhouette_score', 0):.3f}")
        logger.info(f"   ç±»å†…å¹³å‡ç›¸ä¼¼åº¦: {quality.get('avg_similarity_within', 0):.3f}")
        logger.info(f"   ç±»é—´å¹³å‡ç›¸ä¼¼åº¦: {quality.get('avg_similarity_between', 0):.3f}")
        logger.info(f"   èšç±»å¤§å°åˆ†å¸ƒ:")

        # åˆ†æèšç±»å¤§å°åˆ†å¸ƒ
        cluster_sizes = df['cluster_id'].value_counts()
        size_stats = cluster_sizes.describe()

        for stat, value in size_stats.items():
            logger.info(f"     {stat}: {value:.1f}")

        logger.info(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: {Config.OUTPUT_DIR}")
        logger.infologger.info("=" * 60)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='ç™¾ä¸‡çº§åœ°å€ç›¸ä¼¼åº¦è¯†åˆ«ä¸èšç±»ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
      ä½¿ç”¨ç¤ºä¾‹:
      # æµ‹è¯•è¿è¡Œï¼ˆ1ä¸‡æ¡æ•°æ®ï¼‰
      python main-bak.py --input data/sample.csv --output test_run --sample 10000
    
      # å…¨é‡è¿è¡Œ
      python main-bak.py --input data/addresses.csv --output full_run
    
      # æŒ‡å®šåˆ—å
      python main-bak.py --input data/addresses.csv --address-col address --lon-col lng --lat-col lat
        """
    )

    parser.add_argument('--input', required=True,
                        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰')
    parser.add_argument('--output', default='result',
                        help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--address-col', default='address',
                        help='åœ°å€åˆ—å')
    parser.add_argument('--lon-col',
                        help='ç»åº¦åˆ—åï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--lat-col',
                        help='çº¬åº¦åˆ—åï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--sample', type=int,
                        help='é‡‡æ ·å¤§å°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--threshold', type=float, default=0.7,
                        help='ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤: 0.7ï¼‰')
    parser.add_argument('--jobs', type=int, default=28,
                        help='å¹¶è¡Œä»»åŠ¡æ•°ï¼ˆé»˜è®¤: 28ï¼‰')
    parser.add_argument('--visualize', action='store_true',
                        help='ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š')
    parser.add_argument('--config',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    logger.info("ğŸš€ å¯åŠ¨åœ°å€ç›¸ä¼¼åº¦è¯†åˆ«ä¸èšç±»ç³»ç»Ÿ")
    logger.info(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    logger.info(f"è¾“å‡ºå‰ç¼€: {args.output}")
    logger.info(f"å¹¶è¡Œä»»åŠ¡æ•°: {args.jobs}")
    logger.info(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {args.threshold}")

    # åŠ è½½é…ç½®
    config = Config.ALGORITHM_CONFIG.copy()

    # æ›´æ–°å‘½ä»¤è¡Œå‚æ•°
    config['clustering']['similarity_threshold'] = args.threshold
    config['performance']['n_jobs'] = args.jobs

    # å¦‚æœæœ‰é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½å¹¶è¦†ç›–
    if args.config:
        import json
        with open(args.config, 'r', encoding='utf-8') as f:
            user_config = json.load(f)
        config.update(user_config)

    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = AddressSimilaritySystem(config)

    # è¿è¡Œå¤„ç†ç®¡é“
    try:
        system.run_pipeline(
            input_file=args.input,
            output_prefix=args.output,
            address_column=args.address_col,
            lon_column=args.lon_col,
            lat_column=args.lat_col,
            sample_size=args.sample
        )

        # å¦‚æœéœ€è¦å¯è§†åŒ–
        if args.visualize:
            logger.info("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
            from utils.visualization import VisualizationTools
            viz = VisualizationTools()

            # åŠ è½½ç»“æœæ•°æ®
            output_dir = Config.OUTPUT_DIR
            results_files = list(output_dir.glob(f"{args.output}_clusters_*.csv"))

            if results_files:
                latest_file = max(results_files, key=lambda x: x.stat().st_mtime)
                df = pd.read_csv(latest_file)

                # ç”Ÿæˆå¯è§†åŒ–
                viz.create_interactive_report(df, output_prefix=args.output)
                logger.info("âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ")

    except KeyboardInterrupt:
        logger.error("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    exit(main())
